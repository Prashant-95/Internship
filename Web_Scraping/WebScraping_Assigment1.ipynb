{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b75a1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448d26e",
   "metadata": {},
   "source": [
    "### 1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70f2be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Header(url):\n",
    "    #Send get request to web to get required file to scrap\n",
    "    page = requests.get(url)\n",
    "    #Extract page content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    headers = soup.find_all('h2', class_ = 'mp-h2')\n",
    "    headers_list = []\n",
    "    #Add page content to file\n",
    "    for i in headers:\n",
    "        headers_list.append(i.text)\n",
    "    #generate data frame\n",
    "    df = pd.DataFrame({'Headers': headers_list})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e415a991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0  From today's featured article\n",
      "1               Did you know ...\n",
      "2                    In the news\n",
      "3                    On this day\n",
      "4       Today's featured picture\n",
      "5       Other areas of Wikipedia\n",
      "6    Wikipedia's sister projects\n",
      "7            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "Extract_Header('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a44ec3",
   "metadata": {},
   "source": [
    "### 2) Write a python program to display IMDB’s Top rated 50 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92e88a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Top_50_IMDB_Movies(url):\n",
    "    #Send get request to web to get required file to scrap\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    #Extract page content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    #declare emoty lists \n",
    "    titles_list = []\n",
    "    ratings_list = []\n",
    "    Years_of_release_list = []\n",
    "    \n",
    "    #extract all elements\n",
    "    titles = soup.find_all('td', class_ = 'titleColumn')\n",
    "    ratings = soup.find_all('td', class_ = 'ratingColumn imdbRating')\n",
    "    years_of_release = soup.find_all('span', class_ = 'secondaryInfo')\n",
    "    \n",
    "    #Display only 50 movies in top\n",
    "\n",
    "\n",
    "    for name in titles:\n",
    "        titles_list.append(name.text.split('\\n')[2].strip())\n",
    "    for rating in ratings:\n",
    "        ratings_list.append(rating.text.split('\\n')[1])\n",
    "    for year in years_of_release:\n",
    "        Years_of_release_list.append(year.text.removeprefix('(').removesuffix(')'))\n",
    "    #i = i+1\n",
    "        \n",
    "    #generate data frame\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    df = pd.DataFrame({'Titles': titles_list[:50], 'Rating' : ratings_list[:50], 'Year_of_Release': Years_of_release_list[:50]})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0968ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Titles Rating Year_of_Release\n",
      "0                            The Shawshank Redemption    9.2            1994\n",
      "1                                       The Godfather    9.2            1972\n",
      "2                                     The Dark Knight    9.0            2008\n",
      "3                               The Godfather Part II    9.0            1974\n",
      "4                                        12 Angry Men    9.0            1957\n",
      "5                                    Schindler's List    8.9            1993\n",
      "6       The Lord of the Rings: The Return of the King    8.9            2003\n",
      "7                                        Pulp Fiction    8.8            1994\n",
      "8   The Lord of the Rings: The Fellowship of the Ring    8.8            2001\n",
      "9                     Il buono, il brutto, il cattivo    8.8            1966\n",
      "10                                       Forrest Gump    8.8            1994\n",
      "11                                         Fight Club    8.7            1999\n",
      "12              The Lord of the Rings: The Two Towers    8.7            2002\n",
      "13                                          Inception    8.7            2010\n",
      "14                            The Empire Strikes Back    8.7            1980\n",
      "15                                         The Matrix    8.7            1999\n",
      "16                                         GoodFellas    8.7            1990\n",
      "17                    One Flew Over the Cuckoo's Nest    8.6            1975\n",
      "18                                              Se7en    8.6            1995\n",
      "19                              It's a Wonderful Life    8.6            1946\n",
      "20                               Shichinin no samurai    8.6            1954\n",
      "21                           The Silence of the Lambs    8.6            1991\n",
      "22                                Saving Private Ryan    8.6            1998\n",
      "23                                     Cidade de Deus    8.6            2002\n",
      "24                                       Interstellar    8.6            2014\n",
      "25                                    La vita è bella    8.6            1997\n",
      "26                                     The Green Mile    8.6            1999\n",
      "27                                          Star Wars    8.5            1977\n",
      "28                         Terminator 2: Judgment Day    8.5            1991\n",
      "29                                 Back to the Future    8.5            1985\n",
      "30                      Sen to Chihiro no kamikakushi    8.5            2001\n",
      "31                                        The Pianist    8.5            2002\n",
      "32                                             Psycho    8.5            1960\n",
      "33                                       Gisaengchung    8.5            2019\n",
      "34                                               Léon    8.5            1994\n",
      "35                                      The Lion King    8.5            1994\n",
      "36                                          Gladiator    8.5            2000\n",
      "37                                 American History X    8.5            1998\n",
      "38                                       The Departed    8.5            2006\n",
      "39                                           Whiplash    8.5            2014\n",
      "40                                       The Prestige    8.5            2006\n",
      "41                                 The Usual Suspects    8.5            1995\n",
      "42                                         Casablanca    8.5            1942\n",
      "43                                     Hotaru no haka    8.5            1988\n",
      "44                                            Seppuku    8.5            1962\n",
      "45                                   The Intouchables    8.5            2011\n",
      "46                                       Modern Times    8.4            1936\n",
      "47                       Once Upon a Time in the West    8.4            1968\n",
      "48                              Nuovo Cinema Paradiso    8.4            1988\n",
      "49                                        Rear Window    8.4            1954\n"
     ]
    }
   ],
   "source": [
    "Top_50_IMDB_Movies('https://www.imdb.com/chart/top/?ref_=nv_mv_250')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439beed",
   "metadata": {},
   "source": [
    "### 3) Write a python program to display IMDB’s Top rated 50 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1048c2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Titles Rating Year_of_Release\n",
      "0   Ramayana: The Legend of Prince Rama    8.6            1993\n",
      "1            Rocketry: The Nambi Effect    8.4            2022\n",
      "2                               Nayakan    8.4            1987\n",
      "3                              Gol Maal    8.4            1979\n",
      "4                           777 Charlie    8.4            2022\n",
      "5                            Anbe Sivam    8.4            2003\n",
      "6                     Pariyerum Perumal    8.4            2018\n",
      "7                           Apur Sansar    8.4            1959\n",
      "8                              Jai Bhim    8.4            2021\n",
      "9                              3 Idiots    8.4            2009\n",
      "10                     Manichitrathazhu    8.3            1993\n",
      "11                                #Home    8.3            2021\n",
      "12                      Soorarai Pottru    8.3            2020\n",
      "13                         Black Friday    8.3            2004\n",
      "14                    Kumbalangi Nights    8.3            2019\n",
      "15                    C/o Kancharapalem    8.3            2018\n",
      "16                     Taare Zameen Par    8.3            2007\n",
      "17                             Kireedam    8.3            1989\n",
      "18                               Dangal    8.3            2016\n",
      "19                               Kaithi    8.3            2019\n",
      "20                               Jersey    8.3            2019\n",
      "21                                   96    8.3            2018\n",
      "22                          Maya Bazaar    8.2            1957\n",
      "23                            Natsamrat    8.2            2016\n",
      "24                               Asuran    8.2            2019\n",
      "25                           Drishyam 2    8.2            2021\n",
      "26                           Sita Ramam    8.2            2022\n",
      "27                         Thevar Magan    8.2            1992\n",
      "28                           Visaaranai    8.2            2015\n",
      "29                  Sarpatta Parambarai    8.2            2021\n",
      "30                           Thalapathi    8.2            1991\n",
      "31                         Nadodikkattu    8.2            1987\n",
      "32                      Pather Panchali    8.2            1955\n",
      "33                             Drishyam    8.2            2013\n",
      "34                         Thani Oruvan    8.2            2015\n",
      "35                   Jaane Bhi Do Yaaro    8.2            1983\n",
      "36                         Vada Chennai    8.2            2018\n",
      "37                            Aparajito    8.2            1956\n",
      "38                         Sardar Udham    8.2            2021\n",
      "39                    Khosla Ka Ghosla!    8.2            2006\n",
      "40                              Anniyan    8.2            2005\n",
      "41                             Ratsasan    8.1            2018\n",
      "42                        Chupke Chupke    8.1            1975\n",
      "43                   Gangs of Wasseypur    8.1            2012\n",
      "44                             Drishyam    8.1            2015\n",
      "45                              Peranbu    8.1            2018\n",
      "46                       Bangalore Days    8.1            2014\n",
      "47                             Mahanati    8.1            2018\n",
      "48                                Satya    8.1            1998\n",
      "49                               Premam    8.1            2015\n"
     ]
    }
   ],
   "source": [
    "Top_50_IMDB_Movies('https://www.imdb.com/india/top-rated-indian-movies/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=461131e5-5af0-4e50-bee2-223fad1e00ca&pf_rd_r=7YKPS81Q5TE720QFAFK2&pf_rd_s=center-1&pf_rd_t=60601&pf_rd_i=india.toprated&ref_=fea_india_ss_toprated_india_tr_india250_hd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3330f",
   "metadata": {},
   "source": [
    "### 4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "431e0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Former_Presidents_Of_India(url):\n",
    "    #Get the page info\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    #extract full page content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    #define the lists to store values\n",
    "    President_Name_List = []\n",
    "    Term_of_Office_List = []\n",
    "    \n",
    "    #extract Name and term\n",
    "    President_details = soup.find_all('div', class_ = 'presidentListing')\n",
    "    \n",
    "    #Store Names and Terms in lists\n",
    "    for detail in President_details:\n",
    "        President_Name_List.append(detail.text.split('\\n')[1].split('(')[0].removesuffix(' '))\n",
    "        Term_of_Office_List.append(detail.text.split('\\n')[2].split(':')[1])\n",
    "        \n",
    "    #generate and display Data Frame\n",
    "    df = pd.DataFrame({'Name of Respected President' : President_Name_List, 'Term of Office' : Term_of_Office_List})\n",
    "    print(df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff6b2856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name of Respected President  \\\n",
      "0           Shri Ram Nath Kovind   \n",
      "1          Shri Pranab Mukherjee   \n",
      "2   Smt Pratibha Devisingh Patil   \n",
      "3         DR. A.P.J. Abdul Kalam   \n",
      "4           Shri K. R. Narayanan   \n",
      "5        Dr Shankar Dayal Sharma   \n",
      "6            Shri R Venkataraman   \n",
      "7               Giani Zail Singh   \n",
      "8      Shri Neelam Sanjiva Reddy   \n",
      "9       Dr. Fakhruddin Ali Ahmed   \n",
      "10  Shri Varahagiri Venkata Giri   \n",
      "11              Dr. Zakir Husain   \n",
      "12  Dr. Sarvepalli Radhakrishnan   \n",
      "13           Dr. Rajendra Prasad   \n",
      "\n",
      "                                       Term of Office  \n",
      "0                     25 July, 2017 to 25 July, 2022   \n",
      "1                     25 July, 2012 to 25 July, 2017   \n",
      "2                     25 July, 2007 to 25 July, 2012   \n",
      "3                     25 July, 2002 to 25 July, 2007   \n",
      "4                     25 July, 1997 to 25 July, 2002   \n",
      "5                     25 July, 1992 to 25 July, 1997   \n",
      "6                     25 July, 1987 to 25 July, 1992   \n",
      "7                     25 July, 1982 to 25 July, 1987   \n",
      "8                     25 July, 1977 to 25 July, 1982   \n",
      "9                24 August, 1974 to 11 February, 1977  \n",
      "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
      "11                        13 May, 1967 to 3 May, 1969  \n",
      "12                       13 May, 1962 to 13 May, 1967  \n",
      "13                   26 January, 1950 to 13 May, 1962  \n"
     ]
    }
   ],
   "source": [
    "Former_Presidents_Of_India('https://presidentofindia.nic.in/former-presidents.htm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a09e18",
   "metadata": {},
   "source": [
    "### 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame \n",
    "### a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "### b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "### c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94368bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "def Top_10_ODI_Teams(url):\n",
    "    #Get the page info\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    #extract full page content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    #define the lists to store values\n",
    "    teams_List = []\n",
    "    matches_List = []\n",
    "    points_List = []\n",
    "    ratings_List = []\n",
    "    \n",
    "    #extract teams details\n",
    "    teams = soup.find_all('span', class_ = 'u-hide-phablet')\n",
    "    match1 = soup.find('td', class_ = 'rankings-block__banner--matches')\n",
    "    point1 = soup.find('td', class_ = 'rankings-block__banner--points')\n",
    "    rating1 = soup.find('td', class_ = 'rankings-block__banner--rating u-text-right')\n",
    "    matchesandPoints = soup.find_all('td', class_ = 'table-body__cell u-center-text')\n",
    "    ratings = soup.find_all('td', 'table-body__cell u-text-right rating')\n",
    "     \n",
    "    #append list with first element\n",
    "    matches_List.append(match1.text)\n",
    "    points_List.append(point1.text)\n",
    "    ratings_List.append(rating1.text.split('\\n')[1])\n",
    "    \n",
    "    #Create some intermediate lists for operations\n",
    "    matchesandPoint_List = [] #in this list data for both matches and points for othee than fisrt team will be stored together\n",
    "    nonFisrtMatches = [] #in this only matches for other than first rank team will be stored\n",
    "    nonFirstPoints = [] #in this only points for other than first rank team will be stored\n",
    "    \n",
    "    #Sepearte the data for matches and Points\n",
    "    for matchesandpoint in matchesandPoints:\n",
    "        matchesandPoint_List.append(matchesandpoint.text)\n",
    "    for matchorpoint in matchesandPoint_List:\n",
    "        index = matchesandPoint_List.index(matchorpoint)\n",
    "        if(index%2 == 0):\n",
    "            nonFisrtMatches.append(matchorpoint)\n",
    "        else:\n",
    "            nonFirstPoints.append(matchorpoint)\n",
    "    #Store all the datas in seperate lists   \n",
    "    for team in teams:\n",
    "        teams_List.append(team.text)\n",
    "    matches_List.extend(nonFisrtMatches) # extending matches list with first and non first elements\n",
    "    points_List.extend(nonFirstPoints) # extending points List with first and non first elements\n",
    "    for rating in ratings:\n",
    "        ratings_List.append(rating.text)\n",
    "        \n",
    "    #generate and display Data Frame\n",
    "    df = pd.DataFrame({'Top_Teams' : teams_List[:10], 'Matches' : matches_List[:10], 'Points': points_List[:10], 'Ratings' : ratings_List[:10]})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46b7b17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Top_Teams Matches Points                          Ratings\n",
      "0     Australia      35  3,965                              113\n",
      "1   New Zealand      31  3,504                              113\n",
      "2         India      47  5,294                              113\n",
      "3       England      36  3,988                              111\n",
      "4      Pakistan      25  2,649                              106\n",
      "5  South Africa      31  3,141                              101\n",
      "6    Bangladesh      38  3,625                               95\n",
      "7     Sri Lanka      36  3,099                               86\n",
      "8   West Indies      43  3,105                               72\n",
      "9   Afghanistan      20  1,419                               71\n"
     ]
    }
   ],
   "source": [
    "Top_10_ODI_Teams('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "487d9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Top_10_Batsmen(url):\n",
    "    #Get the page info\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    #extract full page content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    #define the lists to store values\n",
    "    players_List = []\n",
    "    team1andrating1_List = []\n",
    "    teams_List = []\n",
    "    ratings_List = []\n",
    "\n",
    "    #extract player details\n",
    "    player1 = soup.find('div', class_ = 'rankings-block__banner--name')\n",
    "    team1andrating1 = soup.find_all('div', class_ = 'rankings-block__banner--nationality')\n",
    "    players = soup.find_all('td', class_ = 'table-body__cell name')\n",
    "    teams = soup.find_all('span', class_ = 'table-body__logo-text')\n",
    "    ratings = soup.find_all('td', class_ = 'table-body__cell u-text-right rating')\n",
    "    \n",
    "    #append list with first element\n",
    "    players_List.append(player1.text)\n",
    "\n",
    "    #store data for team and rating\n",
    "    for teamandrating in team1andrating1:\n",
    "            team1andrating1_List.append(teamandrating.text)\n",
    "    teams_List.append(team1andrating1_List[0].split('\\n')[2])\n",
    "    ratings_List.append(team1andrating1_List[0].split('\\n')[3].strip())\n",
    "    \n",
    "    #Store all the datas in seperate lists   \n",
    "    for player in players:\n",
    "        players_List.append(player.text.split('\\n')[1])\n",
    "    for team in teams:\n",
    "        teams_List.append(team.text)\n",
    "    for rating in ratings:\n",
    "        ratings_List.append(rating.text)\n",
    "        \n",
    "    #generate and display Data Frame\n",
    "    df = pd.DataFrame({'Player' : players_List[:10], 'Teams' : teams_List[:10], 'Ratings': ratings_List[:10]})\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4ffa05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Player Teams Ratings\n",
      "0             Babar Azam   PAK     887\n",
      "1  Rassie van der Dussen    SA     777\n",
      "2            Imam-ul-Haq   PAK     740\n",
      "3           Shubman Gill   IND     738\n",
      "4           David Warner   AUS     726\n",
      "5            Virat Kohli   IND     719\n",
      "6        Quinton de Kock    SA     718\n",
      "7           Rohit Sharma   IND     707\n",
      "8            Steve Smith   AUS     702\n",
      "9           Fakhar Zaman   PAK     699\n"
     ]
    }
   ],
   "source": [
    "Top_10_Batsmen('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5330929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Top_10_Bowlers(url):\n",
    "    #Get the page info\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    #extract full page content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    #define the lists to store values\n",
    "    players_List = []\n",
    "    teams_List = []\n",
    "    ratings_List = []\n",
    "\n",
    "    #extract player details\n",
    "    first_data=soup.find(\"div\",attrs={\"data-cricket-role\":\"bowling\"}).find(\"div\",class_=\"rankings-block__top-player\").text.strip().split(\"\\n\")\n",
    "\n",
    "    other_data=soup.find(\"div\",attrs={\"data-cricket-role\":\"bowling\"}).find_all(\"tr\", class_=\"table-body\")\n",
    "    \n",
    "    #append list with first element\n",
    "    players_List.append(first_data[9])\n",
    "    teams_List.append(first_data[12])\n",
    "    ratings_List.append(first_data[13].strip())\n",
    "    \n",
    "    for data in other_data:\n",
    "        data_List = data.text.strip().split(\"\\n\")\n",
    "        players_List.append(data_List[9])\n",
    "        teams_List.append(data_List[13])\n",
    "        ratings_List.append(data_List[15])\n",
    "    df = pd.DataFrame({\"Top_Bowlers\" : players_List, \"Teams\" : teams_List, \"Ratings\" : ratings_List})\n",
    "    print(df)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa54c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Top_Bowlers Teams Ratings\n",
      "0    Josh Hazlewood   AUS     705\n",
      "1       Trent Boult    NZ     694\n",
      "2    Mohammed Siraj   IND     691\n",
      "3    Mitchell Starc   AUS     686\n",
      "4        Matt Henry    NZ     676\n",
      "5       Rashid Khan   AFG     659\n",
      "6        Adam Zampa   AUS     652\n",
      "7    Shaheen Afridi   PAK     641\n",
      "8  Mujeeb Ur Rahman   AFG     637\n",
      "9   Shakib Al Hasan   BAN     636\n"
     ]
    }
   ],
   "source": [
    "Top_10_Bowlers('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99343fe8",
   "metadata": {},
   "source": [
    "### 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame \n",
    "### a)Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "### b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "### c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3976bf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Top_Teams Matches Points                          Ratings\n",
      "0     Australia      21  3,603                              172\n",
      "1       England      28  3,342                              119\n",
      "2  South Africa      26  3,098                              119\n",
      "3         India      27  2,820                              104\n",
      "4   New Zealand      25  2,553                              102\n",
      "5   West Indies      27  2,535                               94\n",
      "6    Bangladesh      13    983                               76\n",
      "7      Thailand       8    572                               72\n",
      "8      Pakistan      27  1,678                               62\n",
      "9     Sri Lanka       8    353                               44\n"
     ]
    }
   ],
   "source": [
    "#a)\n",
    "Top_10_ODI_Teams('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "244dd971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Player Teams Ratings\n",
      "0         Alyssa Healy   AUS     762\n",
      "1          Beth Mooney   AUS     754\n",
      "2      Laura Wolvaardt    SA     732\n",
      "3       Natalie Sciver   ENG     731\n",
      "4          Meg Lanning   AUS     717\n",
      "5     Harmanpreet Kaur   IND     716\n",
      "6      Smriti Mandhana   IND     714\n",
      "7       Rachael Haynes   AUS     680\n",
      "8  Chamari Athapaththu    SL     655\n",
      "9    Amy Satterthwaite    NZ     641\n"
     ]
    }
   ],
   "source": [
    "#b)\n",
    "Top_10_Batsmen('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "908b368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c)\n",
    "def Top_10_AllRounders(url):\n",
    "    #Get the page info\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    #extract full page content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    #define the lists to store values\n",
    "    players_List = []\n",
    "    teams_List = []\n",
    "    ratings_List = []\n",
    "\n",
    "    #extract player details\n",
    "    first_data=soup.find(\"div\",attrs={\"data-cricket-role\":\"all_round\"}).find(\"div\",class_=\"rankings-block__top-player\").text.strip().split(\"\\n\")\n",
    "\n",
    "    other_data=soup.find(\"div\",attrs={\"data-cricket-role\":\"all_round\"}).find_all(\"tr\", class_=\"table-body\")\n",
    "    \n",
    "    #append list with first element\n",
    "    players_List.append(first_data[9])\n",
    "    teams_List.append(first_data[12])\n",
    "    ratings_List.append(first_data[13].strip())\n",
    "    \n",
    "    for data in other_data:\n",
    "        data_List = data.text.strip().split(\"\\n\")\n",
    "        players_List.append(data_List[9])\n",
    "        teams_List.append(data_List[13])\n",
    "        ratings_List.append(data_List[15])\n",
    "    df = pd.DataFrame({\"Top_Bowlers\" : players_List, \"Teams\" : teams_List, \"Ratings\" : ratings_List})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be973071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Top_Bowlers Teams Ratings\n",
      "0   Hayley Matthews    WI     373\n",
      "1    Natalie Sciver   ENG     371\n",
      "2      Ellyse Perry   AUS     366\n",
      "3    Marizanne Kapp    SA     349\n",
      "4       Amelia Kerr    NZ     336\n",
      "5     Deepti Sharma   IND     322\n",
      "6  Ashleigh Gardner   AUS     292\n",
      "7     Jess Jonassen   AUS     250\n",
      "8          Nida Dar   PAK     232\n",
      "9    Jhulan Goswami   IND     214\n"
     ]
    }
   ],
   "source": [
    "Top_10_AllRounders('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e5a72",
   "metadata": {},
   "source": [
    "### 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\n",
    "### i)Headline\n",
    "### ii) Time\n",
    "### iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf08527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def News_Detail(url):\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    #extract full page content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    #define the lists to store values\n",
    "    Headline_List = []\n",
    "    Time_List = []\n",
    "    NewsLink_List = []\n",
    "\n",
    "        #extract player details\n",
    "    News_Time = soup.find_all(\"span\", class_= \"LatestNews-wrapper\")\n",
    "    Headlines = soup.find_all(\"a\", class_ = \"LatestNews-headline\")\n",
    "\n",
    "    for news in News_Time:\n",
    "        Time_List.append(news.text)\n",
    "    for headline in Headlines:\n",
    "        Headline_List.append(headline.text)\n",
    "        NewsLink_List.append(headline[\"href\"])\n",
    "\n",
    "    df = pd.DataFrame({\"Headline\" : Headline_List, \"Time\" : Time_List, \"News_Link\" : NewsLink_List})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0371974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline         Time  \\\n",
      "0   Black wealth matters: How Ross Mac aims to hel...   11 Min Ago   \n",
      "1   Blockbuster pushed HBO to invest in original c...   37 Min Ago   \n",
      "2   4 side hustles for early birds and night owls:...   41 Min Ago   \n",
      "3   J&J shares rise after company proposes baby po...   51 Min Ago   \n",
      "4   Kia's new EV9 is a 7-passenger electric SUV co...   1 Hour Ago   \n",
      "5    How AI and virtual reality will change stress...   1 Hour Ago   \n",
      "6   Trump lawyer says 'we're not going to get to a...   1 Hour Ago   \n",
      "7   Trump wanted to avoid Stormy Daniels payment b...   1 Hour Ago   \n",
      "8   Wells Fargo's top short-term trade ideas for t...   1 Hour Ago   \n",
      "9   Eli Lilly says experimental Alzheimer's drug r...   1 Hour Ago   \n",
      "10  How Apple CEO Tim Cook clears his head: 'A pal...   1 Hour Ago   \n",
      "11  DOJ announces seizure of cybercrime marketplac...   1 Hour Ago   \n",
      "12  Cardiologist shares 5 foods she eats to lower ...  2 Hours Ago   \n",
      "13  Need more cash? Maybe it's time to start a sid...  2 Hours Ago   \n",
      "14  'The numbers need to come down,' bears say ahe...  2 Hours Ago   \n",
      "15  Ram's electric pickup will top the F-150 Light...  2 Hours Ago   \n",
      "16  ChatGPT is the newest in-demand job skill that...  3 Hours Ago   \n",
      "17  Meta layoffs gut customer service, leaving inf...  3 Hours Ago   \n",
      "18  6 signs you have too much debt—and how to pay ...  3 Hours Ago   \n",
      "19  The box office is catching up to pre-Covid levels  3 Hours Ago   \n",
      "20  Jim Cramer's top 10 things to watch in the sto...  3 Hours Ago   \n",
      "21  Bed Bath & Beyond gets merchandise lifeline as...  3 Hours Ago   \n",
      "22  Wednesday's top analyst calls: Disney, Apple, ...  3 Hours Ago   \n",
      "23  Here's where the A.I. jobs are, according to a...  3 Hours Ago   \n",
      "24  How to avoid a 'stealth tax' on your Social Se...  3 Hours Ago   \n",
      "25  Private payrolls rose by 145,000 in March, wel...  3 Hours Ago   \n",
      "26  Wedbush says buy this coffee stock that could ...  3 Hours Ago   \n",
      "27  Stocks making the biggest moves premarket: Joh...  4 Hours Ago   \n",
      "28  Women are paid less than two-thirds of men's e...  4 Hours Ago   \n",
      "29  Microsoft, Amazon face cloud competition probe...  4 Hours Ago   \n",
      "\n",
      "                                            News_Link  \n",
      "0   https://www.cnbc.com/2023/04/05/how-ross-mac-a...  \n",
      "1   https://www.cnbc.com/2023/04/05/blockbuster-pu...  \n",
      "2   https://www.cnbc.com/2023/04/05/side-hustles-f...  \n",
      "3   https://www.cnbc.com/2023/04/05/johnson-and-jo...  \n",
      "4   https://www.cnbc.com/2023/04/05/kia-ev9-electr...  \n",
      "5   https://www.cnbc.com/2023/04/05/-how-ai-and-vi...  \n",
      "6   https://www.cnbc.com/2023/04/05/trump-new-york...  \n",
      "7   https://www.cnbc.com/2023/04/05/trump-indictme...  \n",
      "8   https://www.cnbc.com/2023/04/05/wells-fargos-t...  \n",
      "9   https://www.cnbc.com/2023/04/05/eli-lilly-says...  \n",
      "10  https://www.cnbc.com/2023/04/05/apple-ceo-tim-...  \n",
      "11  https://www.cnbc.com/2023/04/05/genesis-market...  \n",
      "12  https://www.cnbc.com/2023/04/05/cardiologist-e...  \n",
      "13  https://www.cnbc.com/2023/04/05/need-more-cash...  \n",
      "14  https://www.cnbc.com/2023/04/05/earnings-seaso...  \n",
      "15  https://www.cnbc.com/2023/04/05/ram-1500-rev-e...  \n",
      "16  https://www.cnbc.com/2023/04/05/chatgpt-is-the...  \n",
      "17  https://www.cnbc.com/2023/04/05/meta-layoffs-g...  \n",
      "18  https://www.cnbc.com/2023/04/05/signs-you-have...  \n",
      "19  https://www.cnbc.com/2023/04/05/box-office-alm...  \n",
      "20  https://www.cnbc.com/2023/04/05/cramers-things...  \n",
      "21  https://www.cnbc.com/2023/04/05/bed-bath-beyon...  \n",
      "22  https://www.cnbc.com/2023/04/05/wall-streets-t...  \n",
      "23  https://www.cnbc.com/2023/04/05/ai-jobs-see-th...  \n",
      "24  https://www.cnbc.com/2023/04/05/social-securit...  \n",
      "25  https://www.cnbc.com/2023/04/05/adp-march-2023...  \n",
      "26  https://www.cnbc.com/2023/04/05/wedbush-says-b...  \n",
      "27  https://www.cnbc.com/2023/04/05/stocks-making-...  \n",
      "28  https://www.cnbc.com/2023/04/05/uk-gender-pay-...  \n",
      "29  https://www.cnbc.com/2023/04/05/microsoft-amaz...  \n"
     ]
    }
   ],
   "source": [
    "News_Detail('https://www.cnbc.com/world/?region=world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc99d44",
   "metadata": {},
   "source": [
    "### 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame\n",
    "### i)Paper Title\n",
    "### ii) Authors\n",
    "### iii) Published Date\n",
    "### iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8391356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PaperDetails(url):\n",
    "    #get the web content\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    #Scrape the details\n",
    "    paperTitles = soup.find_all(\"h2\", class_ = \"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\")\n",
    "    authors = soup.find_all(\"span\", class_ = \"sc-1w3fpd7-0 dnCnAO\")\n",
    "    published_Dates = soup.find_all(\"span\", class_ = \"sc-1thf9ly-2 dvggWt\")\n",
    "    urls = soup.find_all(\"a\", class_ = \"sc-5smygv-0 fIXTHm\")\n",
    "\n",
    "    #Define list to store values\n",
    "    paperTitle_List = []\n",
    "    authors_List = []\n",
    "    publishedDates_List = []\n",
    "    paperUrls_List = []\n",
    "    \n",
    "    #append the list with values\n",
    "    for paperTitle in paperTitles:\n",
    "        paperTitle_List.append(paperTitle.text)\n",
    "    for author in authors:\n",
    "        authors_List.append(author.text)\n",
    "    for publishedDate in published_Dates:\n",
    "        publishedDates_List.append(publishedDate.text)\n",
    "    for url in urls:\n",
    "        paperUrls_List.append(url['href'])\n",
    "        \n",
    "    df = pd.DataFrame({\"Paper Title\" : paperTitle_List, \"Authors\" : authors_List, \"Published date\" : publishedDates_List, \"Paper Url\" : paperUrls_List})\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80767239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1                           Making sense of raw input   \n",
      "2   Law and logic: A review from an argumentation ...   \n",
      "3              Creativity and artificial intelligence   \n",
      "4   Artificial cognition for social human–robot in...   \n",
      "5   Explanation in artificial intelligence: Insigh...   \n",
      "6                       Making sense of sensory input   \n",
      "7   Conflict-based search for optimal multi-agent ...   \n",
      "8   Between MDPs and semi-MDPs: A framework for te...   \n",
      "9   The Hanabi challenge: A new frontier for AI re...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11           Argumentation in artificial intelligence   \n",
      "12  Algorithms for computing strategies in two-pla...   \n",
      "13      Multiple object tracking: A literature review   \n",
      "14  Selection of relevant features and examples in...   \n",
      "15  A survey of inverse reinforcement learning: Ch...   \n",
      "16  Explaining individual predictions when feature...   \n",
      "17  A review of possible effects of cognitive bias...   \n",
      "18  Integrating social power into the decision-mak...   \n",
      "19  “That's (not) the output I expected!” On the r...   \n",
      "20  Explaining black-box classifiers using post-ho...   \n",
      "21  Algorithm runtime prediction: Methods & evalua...   \n",
      "22              Wrappers for feature subset selection   \n",
      "23  Commonsense visual sensemaking for autonomous ...   \n",
      "24         Quantum computation, quantum theory and AI   \n",
      "\n",
      "                                              Authors  Published date  \\\n",
      "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
      "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
      "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
      "3                                 Boden, Margaret A.      August 1998   \n",
      "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
      "5                                        Miller, Tim    February 2019   \n",
      "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
      "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
      "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
      "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
      "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
      "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
      "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
      "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
      "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
      "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
      "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
      "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
      "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
      "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
      "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
      "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
      "22                      Kohavi, Ron, John, George H.    December 1997   \n",
      "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
      "24                                   Ying, Mingsheng    February 2010   \n",
      "\n",
      "                                            Paper Url  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "24  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "PaperDetails(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4120b6",
   "metadata": {},
   "source": [
    "### 9) Write a python program to scrape mentioned details from dineout.co.in and make data frame\n",
    "### i) Restaurant name\n",
    "### ii) Cuisine\n",
    "### iii) Location\n",
    "### iv) Ratings\n",
    "### v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c9d2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHotelDeatails(url):\n",
    "    #Get page content\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "\n",
    "    #Scrap rquired details\n",
    "    restaurants = soup.find_all(\"a\", class_ = \"restnt-name ellipsis\")\n",
    "    cuisines = soup.find_all(\"span\", class_ = \"double-line-ellipsis\")\n",
    "    locations = soup.find_all(\"div\", class_ = \"restnt-loc ellipsis\")\n",
    "    ratings = soup.find_all(\"div\", class_ = \"restnt-rating rating-4\")\n",
    "    imageUrls = soup.find_all(\"img\", class_ = \"no-img\")\n",
    "\n",
    "    #Define empty lists\n",
    "    restaurents_List = []\n",
    "    cuisines_List = []\n",
    "    locations_List = []\n",
    "    ratings_List = []\n",
    "    imageUrls_List = []\n",
    "    \n",
    "    #Append data to list\n",
    "    for restaurant in restaurants:\n",
    "        restaurents_List.append(restaurant.text)\n",
    "    for cuisine in cuisines:\n",
    "        cuisines_List.append(cuisine.text.split(\"|\")[1].strip())\n",
    "    for location in locations:\n",
    "        locations_List.append(location.text)\n",
    "    for rating in ratings:\n",
    "        ratings_List.append(rating.text)\n",
    "    for url in imageUrls:\n",
    "        imageUrls_List.append(url[\"data-src\"])\n",
    "        \n",
    "    #Create and print Data Frame\n",
    "    df = pd.DataFrame({\"Restaurant\" : restaurents_List, \"Cuisine\" : cuisines_List, \"Loaction\" : locations_List, \"Ratings\" : ratings_List, \"Image Url\" : imageUrls_List})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8602ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Restaurant                       Cuisine  \\\n",
      "0                   Castle Barbeque         Chinese, North Indian   \n",
      "1                   Jungle Jamboree  North Indian, Asian, Italian   \n",
      "2                        Cafe Knosh          Italian, Continental   \n",
      "3                   Castle Barbeque         Chinese, North Indian   \n",
      "4              The Barbeque Company         North Indian, Chinese   \n",
      "5                       India Grill         North Indian, Italian   \n",
      "6                    Delhi Barbeque                  North Indian   \n",
      "7  The Monarch - Bar Be Que Village                  North Indian   \n",
      "8                 Indian Grill Room         North Indian, Mughlai   \n",
      "\n",
      "                                            Loaction Ratings  \\\n",
      "0                     Connaught Place, Central Delhi       4   \n",
      "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
      "2  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
      "3             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
      "4                 Gardens Galleria,Sector 38A, Noida     3.9   \n",
      "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
      "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
      "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
      "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
      "\n",
      "                                           Image Url  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "GetHotelDeatails(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24f307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
